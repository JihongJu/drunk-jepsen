\section{Conclution}
\label{sec:conclusion}


\begin{itemize}
  \item Feature transferability is robust to mis-segmentation but not to misclassification and inexhaustive segmentation.
  \item Misclassification noises can be alleviated by binarizing/categorizing classes.
  \item Inexhausitive segmentation can be translated as learning with only positive and unlabeled examples.
  \item We proposed a class dependent loss to not over-punish confident positive predictions in the presence of noisy negative labels, and it showed slightly better results than class-weighted loss.
\end{itemize}

% Given the mis-segmentation robustness of feature transferability discussed in section \ref{subsec:robustness}, we proposed to include mis-segmented objects to learn pre-trained model.
% Misclassification noises had also negative impact on weights transferability, but categorizing classes into categories or even binarizing can alleviate the randomness of labels while still achieve comparable fine-tuning performance.
% Lastly, inexhaustive segmentation can have significant influence on feature transferability and it can be compensate partially by modifying the loss function for the deep learning models.
% We proposed a particular class dependent loss in order to not over-punish confident contrary predictions.
% The proposed loss led to better pre-trainig performance as well as better fine-tuning performance with a synthesized dataset as discussed in section \ref{subsec:pulearning}.
% The types of segmentation noises we concentrated on in this study are mis-segmentation, misclassification and inexhaustive segmentation.
