\section{Conclusion}
\label{sec:conclusion}

We investigate in this paper to learn representations by training with segmentation datasets containing label errors.
(1) We report both mislabeled objects and unsegmented objects in a segmentation dataset negatively influence the transferability of the learned representations, i.e., how well the representations transfer to another dataset.
By contrast, false positive segmentations, i.e., segmented objects that are not supposed to be segmented, do not influence the learned representation as significant as the other two types of errors do.
(2) We present that training foreground/background segmentation can produce learned representations comparable to the representations trained with per-class segmentation.
In addition, binarizing classes for segmentation alleviates the negative influence on the learned representations introduced by the mislabeled objects.
(3) We propose a class-dependent sigmoid loss to not over-punish the confident, positive predictions for the negative class when there exist poorly labeled negative samples.
Compared to simply reweighing classes differently, the proposed sigmoid loss for the negative class achieves higher recall while not sacrificing precision by much.
Applying the sigmoid loss to the segmentation model pre-training improves both the segmentation performance and the transferability of the learned representations for a dataset with incomplete segmentations.
