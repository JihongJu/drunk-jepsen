\section{Discussion}
\label{sec:discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% Influence of label noises
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In practice, noisy segmentation dataset may contains not only object label noises but also segmentation errors such as imprecise boundaries, oversegmenting and undersegmenting the objects.
Our method is not able to handle these segmentation noises.
Future investigations for influences of segmentation noises on the learned representation


We assume unsegmented objects and mislabeled objects occur independently to the shape and appearance of objects when we simulate label noises in our experiments.
However, some categories can be more likely to be mislabeled due to its ambiguity in shapes or appearances in practice, such as bear and teddy bear.
Similarly,  ambiguous objects can have a higher probability of being missing than easily recognizable objects.
A real dataset with both clean and noisy segmentation would be valuable to evaluate the influence of noises on feature transferability further.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% Binarizing classes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We proposed to learn representations with binary segmentation when objects mislabelling dominates the pre-training dataset.
Except to segmentation dataset with noisy labels, this method can be potentially helpful when multiple segmentation datasets with are available for pre-training.
In our experiment, the limited amounts of training images, the low-resolution of images and the constraint capacity of the FCN-AlexNet model may explain that more precise labels cannot improve the fine-tuning performance for the pre-trained models.
In general, a trade-off between precise but less ccurate labels and more accurate but less precise labels need to be made to train better transferable features, given a particular dataset.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% sigmoid loss
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We argued that not over-punishing confident, positive predictions for samples with negative labels is beneficial so that we come up with the sigmoid loss for negative examples.
 % by assuming the confident predictions are more likely to be mislabeled given a nonrandom model.
But with the sigmoid loss, we are not able to determine what is the threshold of confident predictions.
A generalized logistic function may be used to replace the normal logistic function as the activation function to achieve a more flexible S-shape and the tuning where the loss saturates.
For example, a parametrized sigmoid loss for the negative class could be $l_{-} = \alpha (\frac{1}{1+\exp{(\beta z_)}})^{\gamma}$, where $z$ is the model output for the negative class, $\alpha$ is the scale factor, $\gamma$ affects where the loss starts, $\beta$ determines where the loss saturates.


Besides, saturating loss for confident predictions has an effect of encouraging confident predictions.
This is intuitively similar to the minimum entropy regularzation for semi-supervised learning.
Both the class-dependent sigmoid loss for PU learning and the minimum regularzation scheme favors low density sepration of input features.
The hard bootstrapping loss by \cite{reed2014training} has a ``soft'' alternative which is equivalent to the minimum entropy regularization.
This explains why the sigmoid loss has a similar performance as the hard bootstrapping loss.

% The sum of sigmoid loss for multiple lacks the probabilistic ground as the sum of logistic loss which is equivalent to minimize the multiplication of independent probabilities $p(y_0 \vert x_0) \cdot p(y_1 \vert x_1) \dots p(y_n \vert x_n)$.
Lastly, applying the sigmoid loss to segmentation data with incomplete segmentations assumes that the pixels for objects are unlabeled independently.
In practice, there is often a spatial dependence for noises in pixel labels which can be valuable to improve performance.
For example, if one or a few pixels have a high probability of being foreground, their neighboring pixels are probably also foreground.

%%%%%%%% confidence
% The problem of weighting negative examples is that it assumes the probability for a negative label being wrong is independent of the underlying distribution of inputs.
% % However, perceptual similar examples are more likely to have the same label, and the negative label is probably wrong if they are assigned contrary labels, supposing the positive labels are always correct.
% Given a nonrandom model, the confidence of a model prediction conveys information about the underlying inputs distribution.
% A confident, positive prediction indicates the example is more likely to be positive than to be negative.
% Therefore, we instead assumed that the probability of mislabeling for a negative label is correlated to the prediction confidence of a trained model.
% In other words, we assumed the confident contrary prediction is more likely to be mislabeled than unconfident examples.

% Upside:
% \begin{itemize}
%   \item treat easy/hard classifications differently
%   \item not over-punish confident positive prediction for negatively labeled examples
% \end{itemize}
%
% Downside:
% \begin{itemize}
%   \item Optimization diffilculty introduced as a result of non-convex objective
% \end{itemize}

% The sigmoid loss was introduced in \cite{tax2016class} to get rid of the effect of outliers.
% In our case, the negative examples given confident predictions by classifier can be considered as outliers.

% Another challenge of difficulty encountered in implementingin extending sigmoid negative loss to multi-class scenario

% \paragraph{The relationship between PU learning and Imbalanced learning}

% Imbalanced
% Easily classified negatives comprise
% the majority of the loss and dominate the gradient. While
% Î± balances the importance of positive/negative examples, it
% does not differentiate between easy/hard examples

% \paragraph{Future works}
%
% \begin{itemize}
%   \item Parameterizing exponential unlabeled loss to determine boundaries of confident and unconfident predictions
%   \item Take into account label noise spatial dependence for neighboring pixels
%   \item Experiment with over-segmentation and under-segmentation noises (negative influence expected)
%   \item Experiment with real datasets
% \end{itemize}

% Trade-offs need to be made between the impact of by label noise and the gain of a larger dataset.
