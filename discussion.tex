\section{Discussion}
\label{sec:discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% Influence of label noises
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We assumed incomplete segmentation, misclassification and false segmentation of objects had occurred independently to the exact shape and appearance of the objects when we synthesized the segmentation noises stochastically in our experiments.
However, some categories can be more likely to be misclassified due to its ambiguity in shapes or appearances in practice, such as bear and teddy bear.
Similarly,  ambiguous objects can have a higher probability of being missing than easily recognizable objects.
A real dataset with both clean and noisy segmentation would be valuable to evaluate the influence of noises on feature transferability further.
% In addtion, we exluded segmenting errors such as imprecise boundaries, oversegmenting or undersegmenting the objects from study because they are a bit more complex to synthesize than the three classification errors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% Binarizing classes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We proposed to binarize or categorize classes to produce accurate but imprecise labels when misclassification of segments dominates a small pre-training dataset.
The result feature transferability depends on whether the inaccurate labels or the imprecise labels is more severe in the decrease of feature transferability.
In our experiment, the limited amounts of training images, the low-resolution of images and the constraint capacity of the FCN-AlexNet model may explain that more precise labels cannot improve the fine-tuning performance for the pre-trained models.
In general, a trade-off between precise but less ccurate labels and more accurate but less precise labels need to be made to train better transferable features, given a particular dataset.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% Sigmoidal loss
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We argued that not over-punishing confident, positive predictions for samples with negative labels can be beneficial by assuming the confident predictions are more likely to be mislabeled given a nonrandom model.
However, there can also be disadvantageous for over-excusing losses for confident predictions.
One typical problem is that a model made bad predictions with confidence will keep emphasizing its wrong predictions.
Therefore, the threshold between punishing and excusing a certain model for make confident contrary predictions need to be tuned appropriately to achieve both high precision and high recall.
Unfortunately, the sigmoidal negative loss has no such hyperparameter to tune, which can be a direction to improve it in the future.
For example, the Focal Loss\cite{lin2017focal} could be a parametrizable alternative of the sigmoidal negative loss.

Besides, applying the sigmoidal negative loss to segmentation assumes that the foreground-to-background mislabeling for each pixel occurred independently.
This assumption made it possible to implement sigmoidal negative loss directly to classification for individual pixels.
However, there is normally a spatial dependence for noises in pixel labels which may help improve the spatial consistency of predictions.
Therefore, future works may be possible to make use of the spatial dependence of pixel label noises.

%%%%%%%% confidence
% The problem of weighting negative examples is that it assumes the probability for a negative label being wrong is independent of the underlying distribution of inputs.
% % However, perceptual similar examples are more likely to have the same label, and the negative label is probably wrong if they are assigned contrary labels, supposing the positive labels are always correct.
% Given a nonrandom model, the confidence of a model prediction conveys information about the underlying inputs distribution.
% A confident, positive prediction indicates the example is more likely to be positive than to be negative.
% Therefore, we instead assumed that the probability of mislabeling for a negative label is correlated to the prediction confidence of a trained model.
% In other words, we assumed the confident contrary prediction is more likely to be mislabeled than unconfident examples.

% Upside:
% \begin{itemize}
%   \item treat easy/hard classifications differently
%   \item not over-punish confident positive prediction for negatively labeled examples
% \end{itemize}
%
% Downside:
% \begin{itemize}
%   \item Optimization diffilculty introduced as a result of non-convex objective
% \end{itemize}

% The sigmoidal loss was introduced in \cite{tax2016class} to get rid of the effect of outliers.
% In our case, the negative examples given confident predictions by classifier can be considered as outliers.

% Another challenge of difficulty encountered in implementingin extending sigmoidal negative loss to multi-class scenario

% \paragraph{The relationship between PU learning and Imbalanced learning}

% Imbalanced
% Easily classified negatives comprise
% the majority of the loss and dominate the gradient. While
% Î± balances the importance of positive/negative examples, it
% does not differentiate between easy/hard examples

% \paragraph{Future works}
%
% \begin{itemize}
%   \item Parameterizing exponential unlabeled loss to determine boundaries of confident and unconfident predictions
%   \item Take into account label noise spatial dependence for neighboring pixels
%   \item Experiment with over-segmentation and under-segmentation noises (negative influence expected)
%   \item Experiment with real datasets
% \end{itemize}

% Trade-offs need to be made between the impact of by label noise and the gain of a larger dataset.
