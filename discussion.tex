\section{Discussion}
\label{sec:discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% Influence of label noises
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We exluded segmenting errors such as inprecise boundaries, oversegmenting or undersegmenting the objects from study because they are a bit more complex to synthesize than the preceding classification errors.

We assumed misclassification error is independent of the exact shape and appearance of the objects, i.e.information from $x$.
This model is often called \textit{noisy at random} \cite{frenay2014classification}.
This assumption does not hold in every cases of practice, for example, some instances can be more likely to be misclassifified due to its ambiguity in shapes or apperances.
But the difficulty of modeling the depence of $x$ leads to simply assuming an input indepence.
Given the class transition probabilities, one can easily synthesize noisy annotations including misclassification errors given a well-annotated segmentation dataset.

%%%%%%%% Binarizing classes
\paragraph{Binarizing/catergoring classes}
\begin{itemize}
  \item Tradeoff between precise bu inaccurate labels and accurate but inprecise labels
  \item The success of region proposal network is a prove of binarized labels can information about ``objectness''.
  \item Though it may depends on the scale of dataset. But since segmentation dataset is often small, it could be fine.
\end{itemize}

%%%%%%%% Exponential loss
\paragraph{The relationship between PU learning and Imbalanced learning}

\paragraph{Upsides and downsides of exponential loss}
% Imbalanced
% Easily classified negatives comprise
% the majority of the loss and dominate the gradient. While
% Î± balances the importance of positive/negative examples, it
% does not differentiate between easy/hard examples
\begin{itemize}
  \item treat easy/hard classifications differently
  \item not over-punish confident positive prediction for negatively labeled examples
  \item easy to implement with the assumption of label noise spatial independence
\end{itemize}


Downside:
\begin{itemize}
  \item Non-parameterizable
  \item Optimization diffilculty introduced as a result of non-convex objective
\end{itemize}


\paragraph{Future works}

\begin{itemize}
  \item Parameterizing exponential unlabeled loss to determine boundaries of confident and unconfident predictions
  \item Take into account label noise spatial dependence for neighboring pixels
  \item Experiment with over-segmentation and under-segmentation noises (negative influence expected)
  \item Experiment with real datasets
\end{itemize}
