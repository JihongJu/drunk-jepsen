\section{Discussion}
\label{sec:discussion}

%%%%%%%% Influence of label noises
We exluded segmenting errors such as inprecise boundaries, oversegmenting or undersegmenting the objects from study because they are a bit more complex to synthesize than the preceding classification errors.

%%%%%%%% Binarizing classes
\paragraph{Binarizing/catergoring classes}
\begin{itemize}
  \item Tradeoff between precise bu inaccurate labels and accurate but inprecise labels
  \item The success of region proposal network is a prove of binarized labels can information about ``objectness''.
  \item Though it may depends on the scale of dataset. But since segmentation dataset is often small, it could be fine.
\end{itemize}

%%%%%%%% Exponential loss
\paragraph{Upsides and downsides of exponential loss}
% Imbalanced
% Easily classified negatives comprise
% the majority of the loss and dominate the gradient. While
% Î± balances the importance of positive/negative examples, it
% does not differentiate between easy/hard examples
\begin{itemize}
  \item treat easy/hard classifications differently
  \item not over-punish confident positive prediction for negatively labeled examples
  \item easy to implement with the assumption of label noise spatial independence
\end{itemize}


Downside:
\begin{itemize}
  \item Non-parameterizable
  \item Optimization diffilculty introduced as a result of non-convex objective
\end{itemize}


\paragraph{Future works}

\begin{itemize}
  \item Parameterizing exponential unlabeled loss to determine boundaries of confident and unconfident predictions
  \item Take into account label noise spatial dependence for neighboring pixels
  \item Experiment with over-segmentation and under-segmentation noises (negative influence expected)
  \item Experiment with real datasets
\end{itemize}
