\section{Related works}
\label{sec:related}

% \paragraph{Semantic Image Segmentation with Deep Neural Nets}
%
% J. Long et.al.  \cite{long2015fully} defined a skip architecture to combine semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations and transfered the learned representations from the contemporray classification networks into fully convolutional networks.
% L. Chen et.al. \cite{chen2016deeplab} removed the last few max pooling layers of the CNNs and upsampled the corresponding filters to avoid the reduced feature resolution by the pooling layers. An additional fully connected Conditional Random Field (CRF) was added to refine the coarse last layer output for better localization performance.
% S. Zheng et.al. \cite{zheng2015conditional} integrate the CRFs-based probabilistic graphical modeling with CNNs in an end-to-end framework.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT Transfer Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Transfer Learning}

%%%%%%%% What Transfer Learning for?

Weights of convolutional neural networks were proved ``transferable'' not only to another dataset, for example, interstitial lung disease (ILD) classification \cite{shin2016deep}, but also to other applications like object detection  \cite{girshick2014rich}, and semantic segmentation\cite{long2015fully}.
Transferable means initializing the model with weights from a pre-trained CNN model results in an improvement of the model performance compared to the random initialization. \cite{pan2010survey}
Yosinski et al. \cite{yosinski2014transferable} discovered that the transferability of features is correlated with feature generality, i.e., how much the feature depends on a particular category.
They also reported the weights from low-level layers of CNN models are well transferable to dissimilar categories, for example, from natural objects to human-made objects.
Because features are transferable regardless the exact categories they are trained with, we argue that binarizing or categorizing the pre-training classes is expected to have no significant influence to the transferability of the result pre-trained models.

% Given the superiority of transferring pre-trained weights and the availability of larger but noisier dataset, we learn transferable features with the noisy dataset and fine-tune the model with small dataset.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT Pre-training with weak supervision
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Apart from the supervised pre-training, one can also perform unsupervised learning to obtain pre-trained features in the absence of labeled training data, typically with auto-encoders \cite{vincent2010stacked,masci2011stacked}, deep belief networks \cite{hinton2006fast,lee2009convolutional}.
%%%%%%%% Deprecated UL in details
% The most common method is to train a generative model with either \textit{auto-encoder} variants or \textit{deep beilief networks}.
% Vincent et al. \cite{vincent2010stacked} trained multiple levels of representation robust to the corrupted inputs with stacked denoising auto-encoders.
% Masci et al. \cite{masci2011stacked} presented a stacked convolutional auto-encoder unsupervised pre-training for hierarchical feature extraction.
% Hinton et al. \cite{hinton2006fast} proposed a greedy learning algorithm to train \textit{deep belief nets} one layer at a time to train hierarchical features.
% Lee et al. \cite{lee2009convolutional} presented a \textit{convolutional deep belief network}, to learn hierachical convolutional representations.
% A few studies \cite{erhan2009difficulty,erhan2010does,bengio2012deep} highlighted the advantage of unsupervised pre-training compared to the random initialization, connecting unsupervised pre-training to a norm of regularization and a method that help disentangle the sample variations.
Though a few studies \cite{erhan2009difficulty,erhan2010does,bengio2012deep} discussed the advantage of unsupervised pre-trained features compared to random weights initialization, the difference between the two has been diminished ever since the arises of modern initialization strategies, namely Xavier initialization \cite{glorot2010understanding} and its variants.
We used random weights initialization as the lower baseline for pre-training with noisy labels.
Representations learned with supervision in the presence of label noises should at least outperform random weights because noisy information should be still better than no information.
% A proper method to learn features in the presence of label noise should at least outperform unsupervised pre-training because noisy information is still better than no information.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT DL with noisy labels
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Deep Learning with Noisy Labels}

The impact of randomly flipped labels on classification performance has been investigated by \cite{sukhbaatar2014training,patrini2016making} for convolutional neural networks.
They both reported decreases in classification performance as the proportion of flipped labels increases for a fixed number of training samples.
On the other hand, Rolnick et al. \cite{rolnick2017deep} argued that deep neural networks can learn robustly from noisy datasets as long as appropriate choices of hyperparameters were made.
They studied the effect of label noise by diluting correct labels with errored labels instead of corrupting correct labels with errored ones and argued that collecting more labels is of more importance than correcting the obtained labels.
None of these studies explored the influence of label noises on feature transferability.
To the best of our knowledge, we are the first research to investigate representations robustness to label noises.

To alleviate the negative effects on classification performance introduced by errored labels, a few methods were proposed for deep neural network models.
Sukhbaatar et al. \cite{sukhbaatar2014training} introduced a linear noise layer on top of the model output, and Patrini et al. \cite{patrini2016making} proposed two forms of loss correction concerning the label observation bias.
Xiao et al. \cite{xiao2015learning} integrated a probabilistic graphic model to an end-to-end deep learning system to predict the observed labels and to correct the observed labels.
Additionally, Reed \& Lee \cite{reed2014training} proposed a bootstrapping loss to emphasize \textit{perceptual consistency} when learning in the presence incomplete and errored labels.
All these methods are proposed to solve label errors from any class to any class but often have the capability of solving specific errors from one class to another.
In our problem of learning with only positive and unlabeled data, the unlabeled data can be treated as a set of examples assigned with correct negative labels and incorrect negative labels.
The problem then converts to learning in the presence of label errors from positive to negative but not from negative to positive.
We modified the bootstrapping loss to interpret the prior knowledge that positive labels are reliable, and set a benchmark in the experiments for the state-of-the-art methods.

%%%%%%%% some more methods
% To study the effect of errored labels, we simulate datasets with random flipped labels from clean labels.
% We also designed our experiments using stochastically simulated noisy segmentations from perfect segmentation.

%%%%%%%% Irrelevant
% Yosinski et al. discovered that feature transferability is negatively affected by the specialization of higher layer neurons and optimization difficulties caused by breaking co-adapted neurons.
% discovered that transferability of a feature is correlated with its generality, i.e. by how much it depends on a particular category.
% Their experiments showed that low-level features, which are less dependent to particular categories, are more transferable than high-level features.
% In general, optimal classification performance on test set often indicates that the extracted features are also optimal, whereas suboptimal classification performance does not necessarily reflect the convolutional features are also suboptimal, especially concerning feature transferability.
% Feature transferability describes the \textit{generality of features}, i.e., the category-independence of features.
% Low-level features were proved to be less dependent to categories and thus more transferable to new tasks than high-level features.  \cite{yosinski2014transferable}
% We experimented in Section \ref{sec:robustness} that how much label noises interfere the transferability of convolutional features.

% Additionally, most of these studies focus on the classification problems, whereas our work inclined more to the semantic segmentation problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT PU Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Positive and Unlabeled Learning}

Traditional methods to learn with only positive samples and unlabeled samples for text classification \cite{liu2003building,li2005learning} often follow a two-step strategy: (1) first identifying a set of reliable negative samples (RN set) from U set and (2) then iteratively build a set of classifiers with RN set and P set, while updating the RN set with a selected classifier.
Methods following this two step strategy do not extend well to deep learning models because it would take tremendously longer time to iteratively train a sequence of deep learning models than to train a sequence of na\"ive Bayesian (NB) models or supported vector machines (SVMs).
For this reason, we do not consider training deep neural networks following this two-step strategy in this work.

Alternatively, one can treat all unlabeled examples as negative, and weight the losses for positive and negative examples differently \cite{lee2003learning}.
Under the assumption that which positive examples are selected to be labeled is completely at random, i.e., independent of the input features, Elkan \& Noto \cite{elkan2008learning} proved that the probability for an object of being observed as positive differs from the probability of being truely positive by a constant factor.
They also observed that a classifier trained on positive and unlabeled examples predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive.
These two works considered only binary classification.
We provide an extension of binary PU learning to multiclass PU learning where examples from multiple relevant classes are partially unlabeled and mixed with examples for the non-relevant class.

The often used logistic loss for neural networks grows to infinity as the confidence of wrong prediction increases to one.
This can be a problem for class-weighed loss: the superfluous penalty for confident, positive predictions, i.e., samples far from the decision boundary have a large influence on the final solution. \cite{tax2016class}
Du et al. \cite{du2015convex} illustrated that the logistic loss and the hinge loss perform worse than the ramp loss in the PU classification setting due to their superfluous penalty for confident predictions.
The non-convex Ramp loss \cite{du2014analysis} and a convex double hinge-loss \cite{du2015convex} were proposed separately to learn from positive and unlabeled data by Du et al.
But neither of the two losses are continuous, which is problematic for a gradient based optimization so that we turns to a continuous altenative of the ramp loss, the sigmoid loss.

Tax \& Wang \cite{tax2016class} proposed to use the sigmoid loss for the positive class to retrieve relevant objects from a large, non-relevant objects dominant dataset, with only poorly labeled relevant objects.
PU learning is happening on an opposite side of this retrieving problem: the positive examples are labeled reliably and the unlabeled examples can be considered as poorly labeled negative examples.
% Lin et al. \cite{lin2017focal} proposed a so-called Focal Loss to down-weight confident predictions and thus focus training on hard negatives for imbalanced learning.
So we proposed to use the sigmoid loss\cite{tax2016class} for the negative class to alleviate the superfluous punishment for confident, positive predictions.
% Using the sigmoid loss for the negative class only is an interpretation of the prior knowledge that the positive labels are reliable while the negative labels are not.
