\section{Related work}
\label{sec:related}

% \paragraph{Semantic Image Segmentation with Deep Neural Nets}
%
% J. Long et.al.\cite{long2015fully} defined a skip architecture to combine semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations and transfered the learned representations from the contemporray classification networks into fully convolutional networks.
% L. Chen et.al.\cite{chen2016deeplab} removed the last few max pooling layers of the CNNs and upsampled the corresponding filters to avoid the reduced feature resolution by the pooling layers. An additional fully connected Conditional Random Field (CRF) was added to refine the coarse last layer output for better localization performance.
% S. Zheng et.al.\cite{zheng2015conditional} integrate the CRFs-based probabilistic graphical modeling with CNNs in an end-to-end framework.

\paragraph{Transfer Learning}
\textit{transfer learning}
We sometimes have a learning task in one domain of interest, but we only have sufficient training data in another domain of interest, where the two domain may or may not share the same feature space and have the same data distribution.
Transfer learning arises in this scenario to transfer knowledge from one domain to anthoer and to improve the performance of learning by avoiding much expensive data-labeling efforts.\cite{pan2010survey}
Recently, a form of knowledge that shows outstanding transferability is the weights of convolutional neural networks.
For example, weights trained on ImageNet images to perform image classification were shown successfully transfered to new categories and new learning problems\cite{girshick2014rich,long2015fully,shin2016deep}.
Convolutional neural networks on images are believed to extract hierarchical features, among which the low-level features look for specific patterns and the high-level features ensemble the information from low-level features.
Low-level features were found more \textit{general}, i.e., less dependent on a particular category, than the high-level features.\cite{yosinski2014transferable}
By training a CNN on a random half of the ImageNet categories and transfer features, varying from the bottom layers to the top layers, to the other half, Yosinski et al. found transferability of features drop due to representation specificity increase.\cite{yosinski2014transferable}
They also found low-level features, especially those of first two layers, presented magnificent transferability to even disimilar categories, even though feature transferability decreased as the distance between the base task and target task increased in general.
% Given the superiority of transferring pre-trained weights and the availability of larger but noisier dataset, we learn transferable features with the noisy dataset and fine-tune the model with small dataset.
% In Section \ref{sec:robustness}, we explored whether transferability of features is robust to annotation errors in the pre-training dataset.
{TODO} Relations to our work.

\paragraph{Unsupervised pre-training}
Apart from supervised pre-training, one can also obtain pre-trained features in an unsupervised or a semi-supervised way.
The most common method is to train a generative model with either \textit{auto-encoder} variants or \textit{deep beilief networks}.
Vincent et al.\cite{vincent2010stacked} trained multiple levels of representation robust to the corrupted inputs with stacked denoising auto-encoders.
Masci et al.\cite{masci2011stacked} presented a stacked convolutional auto-encoder unsupervised pre-training for hierarchical feature extraction.
Hinton et al.\cite{hinton2006fast} proposed a greedy learning algorithm to train \textit{deep belief nets} one layer at a time to train hierarchical features.
Lee et al.\cite{lee2009convolutional} presented a \textit{convolutional deep belief network}, to learn hierachical convolutional representations.
A few studies\cite{erhan2009difficulty,erhan2010does,bengio2012deep} highlighted the advantage of unsupervised pre-training compared to the random initialization, connecting unsupervised pre-training to a norm of regularization and a method that help disentangle the sample variations.
However, better random initialization strategies, for example, xavier initialization\cite{glorot2010understanding} and its variants, have shortened the gap between unsupervised pre-training and random initialization.
Using unsupervised pre-training or not now becomes a tradeoff between the time and resources invested and the performance gain.
Unsupervised deep representation learning is in general not comparable to supervised representation learning especially when large scale dataset is available.
A proper method to learn features in the presence of label noise should at least outperform  unsupervised pre-training because noisy information is still better than no information.

\paragraph{{Deep Learning with Noisy Labels}}
A few studies\cite{sukhbaatar2014training,patrini2016making} investigated the impact of label noise on classification performance with convolutional neural networks assuming the labels were randomly transited from one to another given the probabilities fall in a transition matrix.
They found a significant decrease in classification performance along with the increase of false label proportion when the total number of examples is fixed.
They then proposed methods to handle this label noise at random (NAR)\cite{frenay2014classification} situation by either introducing a linear noise layer on top of the output layer\cite{sukhbaatar2014training} or correcting the loss functions with an estimation of the noise transition matrix\cite{patrini2016making}.
Xiao et al.\cite{xiao2015learning} integrated a probabilistic graphic model to an end-to-end deep learning system to train predicting class labels, either correct or wrong, as well as to correct the wrong labels.
Reed \& Lee\cite{reed2014training} proposed an empirical way of taking into account the \textit{perceptual consistency} for large-scale object recognition and detection when incomplete and noisy labels exist by introducing a bootstrapping modification to the negative log-likelihood, in either a ``Hard'' or a ``soft'' favor.
%Pereyra et al.\cite{pereyra2017regularizing} argued that the maximum entropy principle can prevent the model to have high confident predictions and result in better genralization.

\textit{Noise robustness}
In contrast to the works above, Rolnick et al.\cite{rolnick2017deep} argued that deep neural networks can learn robustly from the noisy dataset as long as an appropriate hyper parameters choice was made.
They studied instead of replacing the correct labels with noisy labels but diluting correct labels with noisy labels to support their argument.
They then concluded sufficiently large training set is of more importance than lower the level of noise.
This work is closely related to our work in Section \ref{sec:robustness}, except that we focus on the label noise robustness regarding the feature transferability instead of the classification performance.
Additionally, most of these studies focus on the classification problems, whereas our work inclined more to the semantic segmentation problem.

\paragraph{Positive and Unlabeled Learning}
If we consider the in-exhaustive annotation issue only, i.e., only a proportion of the target instances were annotated, the problem becomes similar to a so-called \textit{positive and unlabelled learning} (PU learning) setup\cite{li2005learning}.
In the positive and unlabeled learning setup, the training dataset has two sets of examples: the \textit{positive (P) set}, contained only positive examples, and the \textit{unlabeled (U) set}, contained a mix of positive or negative examples.
If we categorize the pixels into either \textit{foreground pixels} or \textit{background pixels}, the correctly annotated instances form the positive set, and the unannotated instances are mixed with the background pixels, forming an unlabeled set.
The previous studies about PU learning mainly focus on the binary classification for linear-separable problems\cite{elkan2008learning,lee2003learning},  whereas we showed in Section \ref{sec:pulearning} that it is possible to train deep neural networks for multiple classes with only ``positive'' and unlabeled examples.
