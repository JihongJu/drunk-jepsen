\section{Related work}
\label{sec:related}

% \paragraph{Semantic Image Segmentation with Deep Neural Nets}
%
% J. Long et.al.\cite{long2015fully} defined a skip architecture to combine semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations and transfered the learned representations from the contemporray classification networks into fully convolutional networks.
% L. Chen et.al.\cite{chen2016deeplab} removed the last few max pooling layers of the CNNs and upsampled the corresponding filters to avoid the reduced feature resolution by the pooling layers. An additional fully connected Conditional Random Field (CRF) was added to refine the coarse last layer output for better localization performance.
% S. Zheng et.al.\cite{zheng2015conditional} integrate the CRFs-based probabilistic graphical modeling with CNNs in an end-to-end framework.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT Transfer Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Transfer Learning}

%%%%%%%% What Transfer Learning for?

Weights of convolutional neural networks were proved ``transferable'' not only to another dataset\cite{shin2016deep,yosinski2014transferable} but also to other applications\cite{girshick2014rich,long2015fully}.
Transferring weights of CNN models for tasks and applications that are different from which the weights were originally trained with allows to improve the performance of learning without engaging in much efforts spent for data-labeling.\cite{pan2010survey}
Yosinski et al.\cite{yosinski2014transferable} reported that initializing a network with transferred features from almost any number of layers can produce a boost to the fine-tuning performance with better generalization.
Transferability is possible between completely dissimilar tasks, e.g., natural classes and man-made classes.
It could also be possible to transfer weights trained with noisy labels.

% Given the superiority of transferring pre-trained weights and the availability of larger but noisier dataset, we learn transferable features with the noisy dataset and fine-tune the model with small dataset.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT Pre-training with weak supervision
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Apart from supervised pre-training, one can also perform unsupervised learning to obtain pre-trained features typically with auto-encoders\cite{vincent2010stacked,masci2011stacked}, deep belief networks\cite{hinton2006fast,lee2009convolutional}.
%%%%%%%% Deprecated UL in details
% The most common method is to train a generative model with either \textit{auto-encoder} variants or \textit{deep beilief networks}.
% Vincent et al.\cite{vincent2010stacked} trained multiple levels of representation robust to the corrupted inputs with stacked denoising auto-encoders.
% Masci et al.\cite{masci2011stacked} presented a stacked convolutional auto-encoder unsupervised pre-training for hierarchical feature extraction.
% Hinton et al.\cite{hinton2006fast} proposed a greedy learning algorithm to train \textit{deep belief nets} one layer at a time to train hierarchical features.
% Lee et al.\cite{lee2009convolutional} presented a \textit{convolutional deep belief network}, to learn hierachical convolutional representations.
% A few studies\cite{erhan2009difficulty,erhan2010does,bengio2012deep} highlighted the advantage of unsupervised pre-training compared to the random initialization, connecting unsupervised pre-training to a norm of regularization and a method that help disentangle the sample variations.
Though a few studies\cite{erhan2009difficulty,erhan2010does,bengio2012deep} discussed the advantage of unsupervised pre-trained features compared to random weights initialization, the distance between the two has been shortened ever since the arises of modern initialization strategies, namely Xavier initialization\cite{glorot2010understanding} and its variants.
We used random weights initialization as the lower bounds for pre-training with noisy labels.
Features learned in the presence of label noises should at least outperform random weights because noisy information should be still better than no information.
% A proper method to learn features in the presence of label noise should at least outperform unsupervised pre-training because noisy information is still better than no information.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT DL with noisy labels
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Deep Learning with Noisy Labels}

The impact of randomly flipped labels on classification performance has been investigated by \cite{sukhbaatar2014training,patrini2016making} for convolutional neural networks.
They both reported decreases in classification performance as the proportion of flipped labels increases for a fixed number of training samples.
By contrast, Rolnick et al.\cite{rolnick2017deep} argued that deep neural networks can learn robustly from the noisy dataset as long as appropriate choices of hyperparameters were made.
They studied the effects of diluting instead of replacing correct labels with noisy labels and concluded that larger training set is of more importance than lower the level of noise.
None of these studies explored the influence of label noises on feature transferability.

Methods, including a linear noise layer on top of the model output\cite{sukhbaatar2014training}, loss correctness with an estimation of the noise transition matrix\cite{patrini2016making} were proposed to compensate the negative effect on classification performance introduced by flipped labels.
We designed our experiments using noisy segmentations synthesized with perfect segmentation as well.

%%%%%%%% some more methods
% Xiao et al.\cite{xiao2015learning} integrated a probabilistic graphic model to an end-to-end deep learning system to train predicting class labels, either correct or wrong, as well as to correct the wrong labels.
%Pereyra et al.\cite{pereyra2017regularizing} argued that the maximum entropy principle can prevent the model to have high confident predictions and result in better genralization.

%%%%%%%% Irrelevant
% Yosinski et al. discovered that feature transferability is negatively affected by the specialization of higher layer neurons and optimization difficulties caused by breaking co-adapted neurons.
% discovered that transferability of a feature is correlated with its generality, i.e. by how much it depends on a particular category.
% Their experiments showed that low-level features, which are less dependent to particular categories, are more transferable than high-level features.
% In general, optimal classification performance on test set often indicates that the extracted features are also optimal, whereas suboptimal classification performance does not necessarily reflect the convolutional features are also suboptimal, especially concerning feature transferability.
% Feature transferability describes the \textit{generality of features}, i.e., the category-independence of features.
% Low-level features were proved to be less dependent to categories and thus more transferable to new tasks than high-level features. \cite{yosinski2014transferable}
% We experimented in Section \ref{sec:robustness} that how much label noises interfere the transferability of convolutional features.

% Additionally, most of these studies focus on the classification problems, whereas our work inclined more to the semantic segmentation problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT PU Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Positive and Unlabeled Learning}

Traditional positive and unlabeled learning methods proposed for text classification\cite{liu2003building} do not extend well to deep learning models.
These traditional methods often follow a two-step strategy: first identifying a set of reliable negative samples (RN set) from U set and then iteratively build a set of classifiers with RN set and P set, while updating the RN set with a selected classifier.
It is unrealistic to train a couple of deep learning models iteratively because it would take significantly longer time to train neural networks than to train a na\"ive Bayesian (NB) model or supported vector machine (SVM).

Alternatively, one can treat all unlabeled examples as negative and simply reweigh positive and negative examples\cite{lee2003learning}.
Under the assumption of randomly labeled positive examples, Elkan \& Noto\cite{elkan2008learning} demonstrated that a classifier trained on positive and unlabeled examples predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive.
These two works considered only binary classification whereas we showed that it is possible to train deep neural networks for multiple classes with only positive and unlabeled examples.

The problem of weighting negative examples is that it assumes the probability for a negative label being wrong is independent of the underlying distribution of inputs.
% However, perceptual similar examples are more likely to have the same label, and the negative label is probably wrong if they are assigned contrary labels, supposing the positive labels are always correct.
Given a nonrandom model, the confidence of a model prediction conveys information about the underlying inputs distribution.
A confident, positive prediction indicates the example is more similar to positive examples than to negative examples.
Therefore, we instead assumed that the probability of being wrong for a negative label is correlated to the prediction confidence of a trained model.
In other words, the confident contrary prediction is more likely to be mislabeled than unconfident examples.
Lin et al.\cite{lin2017focal} proposed a so-called Focal Loss to down-weight confident predictions and thus focus training on hard negatives for imbalanced learning.
Our sigmoidal negative loss follows a similar design as \cite{lin2017focal} to compensate noisy negative labels.
Additionally, Reed \& Lee\cite{reed2014training} proposed a bootstrapping loss to emphasize \textit{perceptual consistency} in training when incomplete and noisy labels exist.
We modified the hard bootstrapping loss
