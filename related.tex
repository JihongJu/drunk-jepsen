\section{Related works}
\label{sec:related}

% \paragraph{Semantic Image Segmentation with Deep Neural Nets}
%
% J. Long et.al.  \cite{long2015fully} defined a skip architecture to combine semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations and transfered the learned representations from the contemporray classification networks into fully convolutional networks.
% L. Chen et.al. \cite{chen2016deeplab} removed the last few max pooling layers of the CNNs and upsampled the corresponding filters to avoid the reduced feature resolution by the pooling layers. An additional fully connected Conditional Random Field (CRF) was added to refine the coarse last layer output for better localization performance.
% S. Zheng et.al. \cite{zheng2015conditional} integrate the CRFs-based probabilistic graphical modeling with CNNs in an end-to-end framework.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT Transfer Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Transfer Learning}

%%%%%%%% What Transfer Learning for?

Weights of convolutional neural networks were proved ``transferable'' not only to another dataset \cite{shin2016deep,yosinski2014transferable} but also to other applications \cite{girshick2014rich,long2015fully}.
Transferring weights of pre-trained CNN models allows to improve the model performance without engaging in much efforts spent for data-labeling. \cite{pan2010survey}
Yosinski et al. \cite{yosinski2014transferable} reported that initializing a network with transferred features from almost any number of layers can produce a boost to the fine-tuning performance with better generalization.
In other words, transferability is possible between completely dissimilar tasks, for example, from natural objects to human-made objects.
If transferring features from dissimilar datasets is possible,  it could also be possible to transfer weights from a noisy dataset.

% Given the superiority of transferring pre-trained weights and the availability of larger but noisier dataset, we learn transferable features with the noisy dataset and fine-tune the model with small dataset.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT Pre-training with weak supervision
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Apart from supervised pre-training, one can also perform unsupervised learning to obtain pre-trained features typically with auto-encoders \cite{vincent2010stacked,masci2011stacked}, deep belief networks \cite{hinton2006fast,lee2009convolutional}.
%%%%%%%% Deprecated UL in details
% The most common method is to train a generative model with either \textit{auto-encoder} variants or \textit{deep beilief networks}.
% Vincent et al. \cite{vincent2010stacked} trained multiple levels of representation robust to the corrupted inputs with stacked denoising auto-encoders.
% Masci et al. \cite{masci2011stacked} presented a stacked convolutional auto-encoder unsupervised pre-training for hierarchical feature extraction.
% Hinton et al. \cite{hinton2006fast} proposed a greedy learning algorithm to train \textit{deep belief nets} one layer at a time to train hierarchical features.
% Lee et al. \cite{lee2009convolutional} presented a \textit{convolutional deep belief network}, to learn hierachical convolutional representations.
% A few studies \cite{erhan2009difficulty,erhan2010does,bengio2012deep} highlighted the advantage of unsupervised pre-training compared to the random initialization, connecting unsupervised pre-training to a norm of regularization and a method that help disentangle the sample variations.
Though a few studies \cite{erhan2009difficulty,erhan2010does,bengio2012deep} discussed the advantage of unsupervised pre-trained features compared to random weights initialization, the distance between the two has been shortened ever since the arises of modern initialization strategies, namely Xavier initialization \cite{glorot2010understanding} and its variants.
We used random weights initialization as the lower baseline for pre-training with noisy labels.
Features learned in the presence of label noises should at least outperform random weights because noisy information should be still better than no information.
% A proper method to learn features in the presence of label noise should at least outperform unsupervised pre-training because noisy information is still better than no information.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT DL with noisy labels
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Deep Learning with Noisy Labels}

The impact of randomly flipped labels on classification performance has been investigated by \cite{sukhbaatar2014training,patrini2016making} for convolutional neural networks.
They both reported decreases in classification performance as the proportion of flipped labels increases for a fixed number of training samples.
On the other hand, Rolnick et al. \cite{rolnick2017deep} argued that deep neural networks can learn robustly from noisy datasets as long as appropriate choices of hyperparameters were made.
They studied the effects of diluting instead of replacing correct labels with noisy labels and concluded that larger training set is of more importance than lower the level of noise.
None of these studies explored the influence of label noises on feature transferability.
To the best of our knowledge, we are the first research to investigate feature robustness label noises.

Methods, including a linear noise layer on top of the model output \cite{sukhbaatar2014training}, loss correctness with an estimation of the noise transition matrix \cite{patrini2016making} were proposed to compensate the negative effect on classification performance introduced by flipped labels.
To study if the proposed methods can alleviate the influence of noisy labels as expected, both works synthesized the random flipped labels from clean labels.
We designed our experiments using stochastically synthesized noisy segmentations from perfect segmentation as well.

%%%%%%%% some more methods
% Xiao et al. \cite{xiao2015learning} integrated a probabilistic graphic model to an end-to-end deep learning system to train predicting class labels, either correct or wrong, as well as to correct the wrong labels.
%Pereyra et al. \cite{pereyra2017regularizing} argued that the maximum entropy principle can prevent the model to have high confident predictions and result in better genralization.

%%%%%%%% Irrelevant
% Yosinski et al. discovered that feature transferability is negatively affected by the specialization of higher layer neurons and optimization difficulties caused by breaking co-adapted neurons.
% discovered that transferability of a feature is correlated with its generality, i.e. by how much it depends on a particular category.
% Their experiments showed that low-level features, which are less dependent to particular categories, are more transferable than high-level features.
% In general, optimal classification performance on test set often indicates that the extracted features are also optimal, whereas suboptimal classification performance does not necessarily reflect the convolutional features are also suboptimal, especially concerning feature transferability.
% Feature transferability describes the \textit{generality of features}, i.e., the category-independence of features.
% Low-level features were proved to be less dependent to categories and thus more transferable to new tasks than high-level features.  \cite{yosinski2014transferable}
% We experimented in Section \ref{sec:robustness} that how much label noises interfere the transferability of convolutional features.

% Additionally, most of these studies focus on the classification problems, whereas our work inclined more to the semantic segmentation problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT PU Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Positive and Unlabeled Learning}

Traditional positive and unlabeled learning methods proposed for text classification \cite{liu2003building} do not extend well to deep learning models.
These traditional methods often follow a two-step strategy: first identifying a set of reliable negative samples (RN set) from U set and then iteratively build a set of classifiers with RN set and P set, while updating the RN set with a selected classifier.
It would take tremendously longer time to train a couple of deep learning models iteratively than to train a sequence of na\"ive Bayesian (NB) models or supported vector machines (SVMs).
Therefore, it would be unrealistic to train CNN models in this manner.

Alternatively, one can treat all unlabeled examples as negative and simply reweigh positive and negative examples \cite{lee2003learning}.
Under the assumption of randomly labeled positive examples, Elkan \& Noto \cite{elkan2008learning} demonstrated that a classifier trained on positive and unlabeled examples predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive.
These two works considered only binary classification whereas we showed that it is possible to train deep neural networks for multiple classes with only positive and unlabeled examples.

The problem of weighting negative examples is that it assumes the probability for a negative label being wrong is independent of the underlying distribution of inputs.
% However, perceptual similar examples are more likely to have the same label, and the negative label is probably wrong if they are assigned contrary labels, supposing the positive labels are always correct.
Given a nonrandom model, the confidence of a model prediction conveys information about the underlying inputs distribution.
A confident, positive prediction indicates the example is more likely to be positive than to be negative.
Therefore, we instead assumed that the probability of mislabeling for a negative label is correlated to the prediction confidence of a trained model.
In other words, we assumed the confident contrary prediction is more likely to be mislabeled than unconfident examples.
Lin et al. \cite{lin2017focal} proposed a so-called Focal Loss to down-weight confident predictions and thus focus training on hard negatives for imbalanced learning.
The sigmoidal negative loss we proposed in Section \ref{sec:pulearning} follows a similar design as  \cite{lin2017focal} to not over-punish confident positive predictions for samples with negative labels.
Additionally, Reed \& Lee \cite{reed2014training} proposed a bootstrapping loss to emphasize \textit{perceptual consistency} in training when incomplete and noisy labels exist.
We modified the hard bootstrapping loss to interpret the prior knowledge that positive labels are reliable.
