\section{Introduction}
\label{introduction}

%%%%%%%% TEXT Lack of perfect annotations
\noindent
\textit{The lack of ``gold standard'' annotations becomes the bottleneck for semantic segmentation.}

\noindent
The state-of-art deep learning algorithms for semantic segmentation \cite{long2015fully} usually suffered from the lack of large-scale annotated dataset.
Most of these methods assume the existence of precise, consistent and exhaustive annotations. However, collecting such perfect segmentations manually on a large scale is expensive and time-consuming.
Millions of images are available through Flikr, ImageNet and many other sources on the Internet but only a few of them \cite{everingham2015pascal,mottaghi2014role,lin2014microsoft}
were well-annotated for semantic segmentation tasks.

\noindent
A typical method to overcome the lack of training samples is to use the pre-trained weights of the convolutional layers from the classification models because collecting the classification annotations is easier to scale up compared to segmentation annotations.
However, the neural network architecture design for image semantic segmentation does not necessarily follow the design of image classification architectures.
For instance, the segmentation models require less subsample pooling layers than the classification models do to keep more local information to locate the object.
The difficulty for training comparable representation of the images in the context of semantic segmentation arises from the lack of ``gold standard'' on a large scale.
If the ``gold standard'' annotation assumption can be released, collecting segmentation annotation would be much easier to scale.
That leads us to explore methods to learn image representation in the presence of annotation noise.



%%%%%%%% TEXT Annotation Errors
\noindent
\textit{This paragraph should explain the difficulties for collecting segmentation annotation, including what could be the main errors in annotation.}

\noindent
The benchmark datasets usually provide perfect segmentations with all instance annotated (exhaustive) and no misannotated instance (precise).
However, it is natural for human beings to make mistakes while annotating due to the lack of expertise, the inherent ambiguity of tasks or unconscious bias.
Enormous efforts are required to correct the mistakes made, including double-checking the annotations over and over again and ensembling opinions from multiple annotators.
Otherwise, these errors may lead to annotations that contain misannotated instances, misclassification of the instances and unannotated instances.



%%%%%%%% TEXT Reasoning problem
\noindent
\textit{This paragraph should reason the idea obtain pre-trained features by learning ``objectness'' with Positive and Unlabeled examples.}

\noindent
For end-to-end models contained convolutional neural networks, the purpose of a learning objective is not only to train a classifier with minimum errors but also to learn satisfying intermediate convolutional filters.
It is clear that training with the noisy annotations would lead to higher segmenting errors than training with the correct annotations if the validation samples had correct annotations. It is nevertheless unclear how the annotation noises influence the learned image representation.

\noindent
That leads to our research questions:
\begin{enumerate}
  \item How to compensate the classification bias introduced by misannotation and inexhaustive annotations with the appropriate prior knowledge.
  \item How do the label noises influence the learned representation.
\end{enumerate}


%%%%%%%% TEXT Noise model
\noindent
\textit{Briefly formulate the problem.}

\noindent
The Semantic Segmentation problem can be considered as per-pixel classification. Each of the pixels is assigned a label of either $0$, indicating a background pixel, or $k \in {1, \ldots, K}$ denoting a foreground pixel corresponding to an instance from one of the $K$ categories.
The aforementioned errors can be interpreted by the pixel label flipping:
\textit{misannotation} is label flipped from $0$ to $k$, \textit{inexaustive annotation} flipped from $k$ to $0$, and \textit{misclassification} flipped from $i$ to $j$, where $i, j, k \in {1, \ldots, K}$.

\noindent
\textit{Misannotating}

\noindent
One hypothesis made in this work is that the \textit{misannotated} instances can still possibly provide information to learn visual representation for objects, assuming the ``objectness'' is shared in the low-level features.
Supposing a dog toy instance is wrongly annotated as a dog, given the ``toy'' is not one of the pre-defined categories while the ``dog'' is, the misannotation error would introduce bias to the classification layer but not necessarily to the convolutional layers, especially the bottom layers.
The bottom-level features are believed to be shared among different categories and thus should be more robust to the misannotation error than the top-level features if the misannotated instance is still visually distinguishable and semantically meaningful.


\noindent
\textit{Inexaustive annotating}

\noindent
The exhaustive annotations, on the other hand, can introduce bias to both the decoding layer and the encoding layers because they negatively contribute to the activations in all the layers.
Therefore, the inexhaustive annotations need to be properly handled given the prior knowledge modeling the missing pattern of the annotations.
Given that we believe any annotated instance provide information, all the foreground pixels that correspond to the annotated instances become reliable and the background pixels may contain both the true background pixels and object pixels unannotated.
That satisfies a Positive and Unlabeled learning setup where the training dataset contains only the positive examples and unlabeled examples that are the mixed of the positive samples and negative samples.



%%%%%%%% TEXT Table of contents
\noindent
\textit{Table of contents}

\noindent
Related works are summarized in the next section.
In Section \ref{sec:objectness} we judge the possibility of learning convolutional representation with misannotations by learning to predict the pixel objectness.
Section \ref{sec:pulearning} explored the methods to compensate the inexaustive annotations in a Positive and Unlabeled Learning setup.
Features learned by predicting the pixel objectness with inexaustive annotations were then validated with experiments described in Section \ref{sec:results}.

% %%%%%%%% TEXT Semantic segmentation is different from classification
% \textit{This paragraph should explain the difference between classification and semantic segmentation.}
%
% %%%%%%%% TEXT Missing positive label is different
% \textit{This paragraph should explain the difference between inexaustive/imprecise and misclassification.}
% Classification with label noise
