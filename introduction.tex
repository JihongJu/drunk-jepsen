\section{Introduction}
\label{introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT Why noise labels?
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent
\textit{Why noisy labels?
This paragraph should discuss the ubiquity of label noise and difficulties of collecting perfect annotations.}

\noindent
The recent success of deep neural networks benefits from the availability of large-scale supervised datasets such as \cite{russakovsky2015imagenet,everingham2015pascal,mottaghi2014role,lin2014microsoft}.
These datasets allow researchers to develop deep neural network models for object recognition\cite{simonyan2014very}, object detection\cite{girshick2014rich}, semantic image segmentation\cite{long2015fully,zheng2015conditional} and other applications assuming the existence of perfect ground-truth segmentation.
However, collecting well-annotated datasets on a large scale can be tremendously expensive and time-consuming in general.
% For example, it took over 70,000 worker hours to create Microsoft COCO dataset which contains 2.5 million labeled objects in 328k images\cite{lin2014microsoft}.
% A significant percentage of these hours were contributed to judge the correctness of annotations.
When annotators work on the tasks, it is natural for them to make mistakes as a result of lack of expertise, inherent ambiguity of tasks or unconscious bias.
Enormous efforts were made in various manners to ensure the correctness of annotations as reported in, for example, \cite{russakovsky2015imagenet,lin2014microsoft}.
Saving efforts made for correctness will result in a noisy dataset but also potentially more annotated images.
Trade-offs need to be made between the impact of by label noise and the gain of a larger dataset.
% In addition, a slight decrease in annotation errors may require extraordinary additional efforts due to the difficulty of identifying errors from a large-scale dataset.
% In some cases, for example in the medical imaging, the ``gold standard'' itself can be ambiguous and cause disagreement among different annotators.
% \footnote{M: But that's OK or not?  This is what probabilities solve...}
For particular tasks, there exist freely available labels as alternatives to manual annotations.
But these labels are often noisy owing to the way they were created.
For example, one can use digital maps, like OpenStreetMap, to segment aerial images.
These segmentations constructed from maps suffer from the incomplete annotation as well as registration problems.\cite{mnih2012learning}
Besides, Pl@ntNet\footnote{https://identify.plantnet-project.org/}, a crowdsourcing platform, provide millions of images of plants and corresponding labels which may or may not be correct.
Strong supervision is therefore required to correct the mistakes, for example double-checking the annotations over and over again and ensembling opinions from multiple annotators.
It is therefore sometimes inevitable for deep neural network models to accept the existence of noisy annotations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT Why feature's noise robustness?
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent
\textit{Why feature's noise robustness?
This paragraph should discuss
1. CNNs arch: hierarchical feature extraction and task-specific layers
2. CNNs based models benefit from transferability of hierarchical feature}

\noindent
Convolutional neural networks (CNNs) based models often contain two principal components: a stack of convolutional layers to extract hierarchical features and a few task-specific layers to fit the specific learning objectives.
%Both parts are trained jointly in an end-to-end manner via back propagation and are important to achieve good performance for a particular task.
\footnote{M: As a non-expert it is rather unclear to me how to identify or design generic layers and task-specific ones, or what it actually means or why it is important to make this distinction.  In addition, I wonder to what extent end-to-end training actually turns generic layers into task-specific ones. J: I think the idea to make this distinction here is to highlight that hierarchical features are reusable for many different applications and tasks but task-specific ones are not as they may have different number of neurons due to the different number of classes, or have even completely different architectures because the learning objectives are different for different app/tasks.}
The convolutional layers were proved ``transferable'' not only to another dataset\cite{yosinski2014transferable} but also to another application\cite{girshick2014rich,long2015fully}.
This feature transferability allows reusing the convolutional features for tasks and applications different from which they were originally trained with in a transfer learning scenario, and it helps to achieve better performance when there are only limited number of training samples\cite{long2015fully}.

\noindent \textit{This paragraph should introduce the idea of studying the impact of annotation noise on feature transferability.}
\noindent
In cases where only a clean but small dataset is available, an extra noisy but large dataset in a similar domain might be helpful as it can be used to pre-train the convolutional features of the CNNs model.
Previous studies\cite{sukhbaatar2014training,patrini2016making} have reported a negative impact of label noises on classification performance, but not yet on convolutional feature transferability.
In general, optimal classification performance on test set often indicates that the extracted features are also optimal, whereas suboptimal classification performance does not necessarily reflect the convolutional features are also suboptimal, especially concerning feature transferability.
Feature transferability describes the \textit{generality of features}, i.e., the category-independence of features.
Low-level features were proved to be less dependent to categories and thus more transferable to new tasks than high-level features. \cite{yosinski2014transferable}
We experimented in Section \ref{sec:robustness} that how much label noises interfere the transferability of convolutional features.

\noindent \textit{Narrow the problem of discussion down to Segmentation}
\noindent
In this paper, we considered three types noises that happen to semantic image segmentation: mis-segmentation, misclassification and inexhaustive segmentation, as described in details in Section \ref{subsec:formulation}.
We chose to study segmentation errors because:
\begin{enumerate}
  \item It is more difficult to correct noisy segmentation than to correct classification noises for object recognition;
  \item Semantic segmentation can be treated as pixel-wise classification so that the existing noise-robust methods can be applied by assuming classifications for each pixels are made independently.
\end{enumerate}


\noindent \textit{Explain why PU learning is relevant}
\noindent
If we consider the inexhaustive segmenation issue only, i.e., the segmentation contains only a proportion of the target instances while leaving the rest unsegmented, the problem becomes similar to a so-called \textit{positive and unlabelled learning} (PU learning) setup\cite{li2005learning}.
In the positive and unlabeled learning setup, the training dataset has two sets of examples: the \textit{positive (P) set}, containing only positive examples, and the \textit{unlabeled (U) set}, containing a mix of positive or negative examples.
The segmented pixels in the presence of inexhaustive segmentation then form the P set and the unsegmented images construct the U set.
Methods to learn with only positive examples and unlabeled examples become relevant because of the difficulty of distinguish object pixels and background pixels in the U set.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT Table of contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent \textit{Table of contents}

\noindent
In the next section, we summarize related works in areas of transfer learning and learning with noisy labels for deep learning.
In Section \ref{sec:robustness} we formulate the segmentation errors into three categories, mis-segmentation, misclassification and incomplete annotation, and test feature transferability against them separately.
In Section \ref{sec:pulearning} we connect training with inexhaustive segmentations to positive and unlabeled (PU) learning.
We experiment with synthesized noisy dataset in Section \ref{subsec:robustness} to study whether the mis-segmentation, misclassification and inexhaustive segmentation noises have impacts on learning transferable features.
Section \ref{subsec:pulearning} contains experiments for methods to learn with positive and unlabled examples.
Conclusions are summarized in section \ref{sec:conclusion}.

%Features learned by predicting the pixel objectness with inexaustive annotations were then validated with experiments described in Section \ref{sec:results}.


% %%%%%%%% TEXT Semantic segmentation is different from classification
% \textit{This paragraph should explain the difference between classification and semantic segmentation.}
%
% %%%%%%%% TEXT Missing positive label is different
% \textit{This paragraph should explain the difference between inexaustive/imprecise and misclassification.}
% Classification with label noise

%Millions of images are available through Flikr, ImageNet and many other sources on the Internet but only a few of them \cite{everingham2015pascal,mottaghi2014role,lin2014microsoft} have been well-annotated for semantic segmentation tasks.
%The largest segmentation dataset, COCO2014\cite{lin2014microsoft}, contains annotations for only 164,000 training images, smaller by a factor of 10 than the ILSVRC2012 dataset with 1,281,167 labeled training images.
