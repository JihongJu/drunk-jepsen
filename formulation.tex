\section{Problem Formulation}
\label{sec:formulation}

%%%%%%%% TEXT Noise model
\noindent
\textit{Briefly formulate the problem.}

\noindent
Semantic segmentation and other dense prediction tasks can be considered as pixel-wise classification problem.
Supposing there are $K$ pre-defined categories to segment, each of the pixels is assigned a label $k \in [0, K]$, where $k=0$ if the pixel is a pixel for background, or $k \in [1, k]$ if the pixel is correspondent to an object from one of the $K$ categories.
The three types of annotation errors considered in this work are:
\begin{description}
  \item [Misannotation] There are objects, which are semantically meaningful but not not from the pre-defined categories, wrongly segmented as one of the pre-defined categories. The objects have pixel labels flipped from $0$ to $k$.
  \item [Misclassification] There are objects misclassified from one pre-defined category to another. Labels of the corresponding pixels flipped from $i$ to $j$.
  \item [Inexaustive annotation] There are objects from the pre-defined categories unsegmentated. Pixels for the unannotated objects have labels flipped from $k$ to $0$.
\end{description}
where $i, j, k \in [1, \ldots, K]$.

\noindent
\textit{Why would low-level features robust to misannotation}
\noindent
\paragraph{Misannotation}
Convolutional neural networks on images are believed to extract hierarchical features, among which the low-level features look for specific patterns and the high-level features ensemble information from the low-level features.
Low-level features were found\cite{yosinski2014transferable} less dependent on a particular category, i.e. more \textit{general}, than the high-level features by showing the transferability from one category to another.
Supposing a toy dog is wrongly annotated as a dog, given that the ``dog'' is one of the target categories whereas the ``toy'' is not, the error could mislead the model to predict this toy dog as dogs.
However, a toy dog and a dog still look similiar to each other, especially in terms of the visual details: they are both furry, have similiar shape and appearance for eyes, noses, mouths, etc.
These similiarities in features between the toy dog and dog can help learn the low-level features when a toy dog was misannotated as a dog.
If we consider further lower features, for exmaple those for detecting edges and corners, they are commonly transferable to many categories, not necessarily ones that look alike.
dissimilar categories (man-made classes and natural classes)
%If we constraint ourselves to perceive only part of the dog and the toy dog, for example the ears, it may become difficult even for us to distinguish the two because given only the local information there is not much difference between them.
The generality of the low-level features can introduce the robustness to misannotations regarding the transferability when we want to transfer hieraychical features for semantic segmentation in the presence misannotated objects because the low-level features are not dependent on the exact category assigned to the objects.
This annotation error robustness for the low-level features could result in noise robustness for the multiple level pre-trained weights for semantic segmentation with noisy labels.
In Section \ref{sec:objectness}, we tested whether object misannotation had an impact on the transfehen we transfer the learned features to a new dataset with new categories.
That leads us to the following research question:
\begin{enumerate}
  \item How do misannotation and misclassification of instances influence the ``transferability'' of the learned features respectively?
\end{enumerate}
We experimented, in Section \ref{sec:objectness}, how transferable the learned features are in two special cases for the two types of annotation error:
\begin{description}
  \item [instance misannotation] all instances from the non-target categories are misannotated as instance from the target category
  \item [instance misclassification] the classes for all the instances are completely randomly assigned
\end{description}
\footnote{M: So, to what extent is this the actual research question that you would like to answer? J: I think this section answers your question now.}
The \textit{transferability} of the features can be evaluated by how much they can boost the performance of training a new dataset. \cite{yosinski2014transferable}
\textit{TODO One sentence summarizes the results.}


\noindent
\textit{Misclassification}
Misclassification is similiar to misannotation except

\noindent
\textit{Inexaustive annotating}

\noindent
The exhaustive annotations can introduce bias to both the decoding layer and the encoding layers because they negatively contribute to the activations in all the layers.
\footnote{J: This argument need evidence too, or experiment/discussions in details in Section \ref{sec:pulearning}}
The inexhaustive annotations need to be properly handled given the prior knowledge modeling the missing pattern of the annotations.
Given that we believe any annotated instance provide information, all the foreground pixels that correspond to the annotated instances become reliable and the background pixels may contain both the true background pixels and object pixels unannotated.
That satisfies a Positive and Unlabeled learning setup where the training dataset contains only the positive examples and unlabeled examples that are the mixed of the positive samples and negative samples.
