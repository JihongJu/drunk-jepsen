\documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}

% Include other packages here, before hyperref.
\usepackage{subfig}
\newcommand{\subsubfloat}[2]{%
  \begin{tabular}{@{}c@{}}#1\\#2\end{tabular}%
}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{multirow}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}


\begin{document}
%%%%%%%%% Preface
\onecolumn
\pagenumbering{gobble}
\include{preface}

%%%%%%%%% TITLE
\twocolumn
\pagenumbering{arabic}
\newpage
\title{Learn transferable representations in the presence of noisy labels for segmentation}

\author{Jihong Ju\\
Faculty of Electrical Engineering, Mathematics and Computer Science \\
Mekelweg 4, 2628 CD Delft\\
{\tt\small j.ju@student.tudelft.nl}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}


%%%%%%%%% ABSTRACT
\begin{abstract}

Transferring pre-trained representations is widely adopted by convolutional neural networks for semantic segmentation because the training data is often available only on a small scale.
In domains where directly adapting classification networks is challenging, we propose to train representations with segmentation datasets containing mislabeled objects and unsegmented objects.
Our experiments demonstrate that both mislabeled segments and incomplete segmentation lower the fine-tuning performance of the learned representations.
To get rid of the negative effect of objects label noises, we propose to assign objects of any categories a foreground label instead of the exact object categories.
Learning representations by segmenting foreground and background turns out to improve the fine-tuning performance significantly when label noises are dominant in the pre-training data.
In the existence of unsegmented objects, a sigmoid loss for the background class is proposed to achieve high recall while keeping the precision better than simply weighting the classes.
The proposed class dependent, sigmoid loss achieves both better pre-training performance and better fine-tuning performance than the class-weighted loss in the presence of incomplete segmentation.

% % Noisy data exists
% % Transfer learning
% % Noises affect feature transferability
% % Binarizing classes
% % Modify loss for incomplete segmentation
%
% In some domains of interest, there exist noisy segmentations in addition to a limited amount of perfect segmentations.
% Noisy segmentations can, for instance, come from crowdsourcing platforms.
% The existence of noisy datasets motivates us to consider if it is possible to utilize noisy segmentations in training better image segmentation models with limited training samples.
% We focus on to pre-train robustly transferable feature representations with potentially incomplete, mislabeled segmentations.
% In this paper, we investigate the influence of these segmentation noises on model transferability in an experimental setup with synthesized noises.
% With observing a negative impact of objects mislabeling on feature transferability, we discover that combining object categories as one foreground class improves feature transferability for a small training set corrupted heavily with random labels.
% Besides, a sigmoid loss for the background class is proposed to balance precision and recall when training with incomplete segmentations.
% % Compared to simply changing class weights, the proposed sigmoid loss down-weights the losses for confident predictions and unconfident predictions differently.
% Experiments demonstrate that replacing the cross-entropy loss with a sigmoid loss for the background class achieves better pre-training and fine-tuning performances in the presence of incomplete segmentation.
%

\end{abstract}

%%%%%%%%% BODY TEXT

\input{introduction}

\input{related}

\input{robustness}

\input{pulearning}

\input{experiments}

\input{discussion}

\input{conclusion}


{\small
\bibliographystyle{plain}
\bibliography{references}
}

\clearpage
\appendix
\input{appendix_a}
\input{appendix_b}
% \input{appendix_c}
\input{appendix_de}

\end{document}
