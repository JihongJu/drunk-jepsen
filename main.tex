\documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}

% Include other packages here, before hyperref.
\usepackage{subfig}
\newcommand{\subsubfloat}[2]{%
  \begin{tabular}{@{}c@{}}#1\\#2\end{tabular}%
}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{multirow}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}


\begin{document}
%%%%%%%%% Preface
\onecolumn
\pagenumbering{gobble}
% \include{preface}

%%%%%%%%% TITLE
\twocolumn
\pagenumbering{arabic}
\newpage
\title{Learn representations in the presence of segmentation label noises}

\author{Jihong Ju\\
Computer Vision Lab, TU Delft \\
Mekelweg 4, 2628 CD Delft\\
{\tt\small j.ju@student.tudelft.nl}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}


%%%%%%%%% ABSTRACT
\begin{abstract}

Training data for segmentation tasks are often available only on a small scale.
Transferring learned representations from pre-trained classification models is therefore widely adopted by convolutional neural networks for semantic segmentation.
In domains where the representations from the classification models are not directly applicable, we propose to train representations with segmentation datasets that potentially contains label errors.
Our experiments demonstrate that label errors, such as mislabeled segments and missing segmentations, have negative influences to the learned representations.
To alleviate the negative effects of object mislabelling, we propose to discard the object labels and instead train foreground/background segmentation.
The learned representations with binary segmentation achieve a fine-tuning performance comparable to the representations learned with ``gold'' standard segmentations.
In the existence of missing segmentations, a sigmoid loss for the background class is proposed to achieve high recall while keeping the precision better than simply weighting the classes.
The proposed class dependent, sigmoid loss obtains better segmentation performance as well as better representations than the weighting the classes in the presence of missing segmentations.
To summerize, we propose to learn representations with foreground/background segmentation and with a sigmoid loss for the background class when there exist missing segmentations for objects.

% % Noisy data exists
% % Transfer learning
% % Noises affect feature transferability
% % Binarizing classes
% % Modify loss for incomplete segmentation
%
% In some domains of interest, there exist noisy segmentations in addition to a limited amount of perfect segmentations.
% Noisy segmentations can, for instance, come from crowdsourcing platforms.
% The existence of noisy datasets motivates us to consider if it is possible to utilize noisy segmentations in training better image segmentation models with limited training samples.
% We focus on to pre-train robustly transferable feature representations with potentially incomplete, mislabeled segmentations.
% In this paper, we investigate the influence of these segmentation noises on model transferability in an experimental setup with synthesized noises.
% With observing a negative impact of objects mislabeling on feature transferability, we discover that combining object categories as one foreground class improves feature transferability for a small training set corrupted heavily with random labels.
% Besides, a sigmoid loss for the background class is proposed to balance precision and recall when training with incomplete segmentations.
% % Compared to simply changing class weights, the proposed sigmoid loss down-weights the losses for confident predictions and unconfident predictions differently.
% Experiments demonstrate that replacing the cross-entropy loss with a sigmoid loss for the background class achieves better pre-training and fine-tuning performances in the presence of incomplete segmentation.
%

\end{abstract}

%%%%%%%%% BODY TEXT

\input{introduction}

\input{related}

\input{formulation}

\input{pulearning}

\input{experiments}

\input{discussion}

\input{conclusion}

{\small
\bibliographystyle{plain}
\bibliography{references}
}

\clearpage
\appendix
\input{appendix_a}
\input{appendix_b}
% \input{appendix_c}
\input{appendix_de}

\end{document}
