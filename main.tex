\documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}

% Include other packages here, before hyperref.
\usepackage{subfig}
\newcommand{\subsubfloat}[2]{%
  \begin{tabular}{@{}c@{}}#1\\#2\end{tabular}%
}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{multirow}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}


\begin{document}
%%%%%%%%% Preface
\onecolumn
\pagenumbering{gobble}
\include{preface}

%%%%%%%%% TITLE
\twocolumn
\pagenumbering{arabic}
\newpage
\title{Learn transferable features with noisy segmentation datasets}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}


%%%%%%%%% ABSTRACT
\begin{abstract}

% Noisy data exists
% Transfer learning
% Noises affect feature transferability
% Binarizing classes
% Modify loss for incomplete segmentation

The broad existence of noisy datasets motivates us to consider if it is possible to utilize noisy segmentations in training better image segmentation models with limited training samples.
We investigated in this work to pre-train convolutional neural network (CNN) models with potential incomplete, mislabeled segmentations.
The influence of segmentation noises on model transferability was investigated in an experimental setup with synthesized segmentation noises.
With the same setup, we discovered that binarizing classes as foreground and background could improve feature transferability for a small training set corrupted heavily with random labels.
A class-dependent modification to the cross-entropy loss was proposed to balance precision and recall for training with incomplete segmentations.
Compared to simply changing class weights, the proposed sigmoidal loss down-weights the losses for confident predictions and unconfident predictions differently.
Experiments demonstrated that replacing the cross-entropy loss with a sigmoidal loss for background class provide an improvement to both pre-training and fine-tuning performance in the presence of incomplete segmentation.\end{abstract}

%%%%%%%%% BODY TEXT

\input{introduction}

\input{related}

\input{robustness}

\input{pulearning}

\input{experiments}

\input{discussion}

\input{conclusion}


{\small
\bibliographystyle{plain}
\bibliography{references}
}

\clearpage
\input{appendices}

\end{document}
