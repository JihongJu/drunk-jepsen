\documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}

% Include other packages here, before hyperref.
\usepackage{subfig}
\newcommand{\subsubfloat}[2]{%
  \begin{tabular}{@{}c@{}}#1\\#2\end{tabular}%
}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{multirow}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}


\begin{document}
%%%%%%%%% Preface
\onecolumn
\pagenumbering{gobble}
\include{preface}

%%%%%%%%% TITLE
\twocolumn
\pagenumbering{arabic}
\newpage
\title{Learn transferable features with noisy segmentation datasets}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}


%%%%%%%%% ABSTRACT
\begin{abstract}
{TODO: brief introduction}
{TODO: brief introduction}
{TODO: brief introduction}
{TODO: brief introduction}
{TODO: brief introduction}
{TODO: brief introduction}
{TODO: brief introduction}
{TODO: brief introduction}
{TODO: brief introduction}
{TODO: brief introduction}
{TODO: brief introduction}
{TODO: brief introduction}
{TODO: brief introduction}
{TODO: brief introduction}
We invested in this work the influence of segmentation noises on transferability of pre-trained convolutional neural network (CNN) models.
We found binarizing classes as foreground and background can improve feature transferability with a small training set corrupted heavily with random labels.
We modified the cross-entropy loss by down-weighting the loss of very positive predictions to balance precision and recall in the presence of mislabeled positive examples.

\end{abstract}

%%%%%%%%% BODY TEXT

\input{introduction}

\input{related}

\input{robustness}

\input{pulearning}

\input{experiments}

\input{discussion}

\input{conclusion}


{\small
\bibliographystyle{plain}
\bibliography{references}
}

\clearpage
\input{appendices}

\end{document}
