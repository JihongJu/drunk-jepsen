\subsection{Examples}

% It is challenging to find a dataset with both clean and noisy labels available, so we simulate segmentation noises with correct labels.
% A straightforward way to simulate noisy labels is to corrupt true labels stochastically for each segment with a corruption model.
% The corruption model simply describes the probability of the observed labels given the true labels.
% % $$p(\tilde{y_{ij}} \vert x, y_{ij}, e_{ij})$$
% % where the binary error occurence $e_{ij}$ depends on the inputs $x$ and true labels $y$.
%
% For segmentation problems, each pixel (or voxel for 3D segmentation) of a training image has a label assigned to one of the pre-defined categories.
% Supposing there are $K$ pre-defined categories, the label of a pixel in the $i$-th row and $j$-th column from an image of size $h \times w$:
% \[
%   y_{ij} =
%     \begin{cases}
%       1 \leq k \leq K, & \text{for foreground pixels} \\
%       0, & \text{for background pixels}
%     \end{cases}
% \]
% where $1 \leq i \leq h, 1 \leq j \leq w$ and $i,j,k \in \mathrm{Z}^+$.


\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
\end{center}
   \caption{Example of caption.  It is set in Roman so that mathematics
   (always set in Roman: $B \sin A = A \sin B$) may be included without an
   ugly clash.}
\label{fig:long}
\label{fig:onecol}
\end{figure}


\begin{figure*}
\begin{center}
\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
\end{center}
   \caption{Example of a short caption, which should be centered.}
\label{fig:short}
\end{figure*}


{\small\begin{verbatim}
   \usepackage[dvips]{graphicx} ...
   \includegraphics[width=0.8\linewidth]
                   {myfile.eps}
\end{verbatim}
}

\begin{table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Method & Frobnability \\
\hline\hline
Theirs & Frumpy \\
Yours & Frobbly \\
Ours & Makes one's heart Frob\\
\hline
\end{tabular}
\end{center}
\caption{Results.   Ours is better.}
\end{table}


\subsection{segmentation}

%%%%%%%% Why segmentation?
\begin{enumerate}
  \item It is more difficult to correct noisy segmentation than to correct classification noises for object recognition;
  \item Semantic segmentation can be treated as pixel-wise classification so that the existing noise-robust methods can be applied by assuming classifications for each pixels are made independently.
\end{enumerate}

%%%%%%%% Why general features can be robustness?
Convolutional neural networks for processing images are believed to extract a rich hierarchy of visual features that vary from simple to complex and from local to global.\cite{girshick2014rich}
Neurons in the lower hierarchy capture simple patterns such as edges, textures or material properties within small receptive fields, whereas the high-level features capture conceptual information such as people and text in larger receptive fields.
It is natural that simple patterns can be found for many categories whereas conceptual features are often specific to particular categories.
% By training a CNN with half of the ImageNet categories and transfer features to the other half, Yosinski et al.\cite{yosinski2014transferable} found transferability of features decreases from the bottom layers to the top layers, indicating that features change gradually from category independent to class dependent.
Additionally, filters from the first convolutional layer are often observed as Gabor filters or color blobs regardless the training dataset and target categories.\cite{zeiler2014visualizing,lee2009convolutional,krizhevsky2012imagenet,shin2016deep}
Correspondingly, low-level filters, especially ones from the first two layers, were proved well-transferable to dissimilar categories, as reported in \cite{yosinski2014transferable} from natural objects to man-made objects.
These category-independent convolutional filters are denoted \textit{general} in the sense that they are generally usable for many classes.

Given the evidence that some convolutional filters can be applied regardless the exact categories they were trained, we wondered if they can also be learned in a more general way than using the exact labels of specific classes.
For example, labels assigned examples from categories that are visually similar can be inconsistent in practice if not additional human efforts made for correctness.
These noises in labels may have a significant negative influence to the top layers which are sensitive to the given labels but not necessarily to the bottom layers which have already shown class independence to some extent.
If we train models in a transfer learning scenario where we only care about the transferability of a pre-trained model rather than performance achieved on the pre-training task, a label noise robustness of weights transferability can be beneficial to save additional human efforts for label correction.


\noindent
The different levels of hierarchical features in CNNs are believed to play different roles in extracting information from the images.
The low-level features process the local information within small neighborhood and the high-level features ensemble information from lower-level features to extract abstract information.
The high-level features were found to significantly dependent on the exact categories compared to the low-level features which show extraordinary category independence.\cite{yosinski2014transferable}
Previous studies\cite{sukhbaatar2014training,patrini2016making} have shown that training with noisy labels can lead to significantly higher classification errors than training with clean labels if the total number of training samples are fixed.
It is nevertheless unclear how the annotation errors would influence the learned multiple level features.
We made a hypothesis that annotation errors do not necessarily lead to a ``bad representation'' because the ``generality'' of low-level features may contribute to a robustness to the errors when we transfer the learned features to a new dataset with new categories.

\footnote{M: We hypothesize?
For the rest I find the actual hypothesis pretty vague.  Firstly, you use quotation marks quite often, which doesn't make ``things'' clearer.  Either don't use that and make sure that the words you use are indeed the words you like to say or expand the part between ``''s and use some more sentences to really explain what you have in  mind.  Secondly, for me the part ``contribute to a robustness to the errors'' needs further explaining.  You might not have to get mathematically precise here, but I don't understand what you want to say here...}

\noindent \textit{Narrow the problem of discussion down to Segmentation}
The state-of-art DCNN based semantic image segmentation models relies on transferring the pre-trained convolutional filters as well.\cite{long2015fully}
A typical method to pre-train the convolutional filters is to train a classification model with the large-scale ILSVRC dataset \cite{russakovsky2015imagenet}
However, this method constrains the semantic image segmentation models to have the same CNN architecture as the image classification models.
The CNN design for semantic image segmentation does not necessarily follow the design of image classification architectures.
The segmentation models need both global and local information to predict the category and give a fine segmentation, whereas the classification models care less about local information for object localization.
For instance, the presence of the max-pooling layers enable the following convolutional filters to have larger receptive fields but, at the same time, reduce the resolution of the features.
\footnote{M: Is it the presence of max-pooling or the presence of subsampling that enables larger receptive fields?  And what do you mean by resolution?  Should low resolution always mean reduced / ``thrown away'' information? J: Max-pooling is a particular type of subsumpling. Actually both the conv layers and the pooling layers lead to gradually larger receptive field.}
Additional upsampling layers can recover the shape of the output segmentation but cannot fully recover the information dropped by subsampling.
This first-pooling-and-then-upsampling pipeline can result in coarse segmentation output \cite{chen2016deeplab} with non-shape boundaries and blob-like shapes.
\footnote{M: Also for this claim, I think we need a reference or something like that. Or other proof indeed...  In addition, I wonder whether we can explain why this may be the case? J: Yes, I can refer to a paragraph in the intro of CRFasRNN and enrich the discussion a bit.}
%The coarse output can be refined with Conditional Random Field (CRF) inference\cite{zheng2015conditional,chen2016deeplab}.
\noindent
Alternatively, one can also extract convolutional features with semantic segmentation tasks.
But it is more difficult to collect well-annotated dataset for semantic image segmentation than for object recognition.
A large number of annotated images are of great value to train sufficiently generalized representation and avoid overfitting, given the ``data-hungry'' nature of DCNNs.
\footnote{M: For the rest a bit vague... what do we really mean by suffer? Maybe this becomes clear later in the intro...? J: Refrased. M: This is still quite unclear to me.  I would interpret ``data-hungry'' as an NN that overtrains even with large amount of data, but one would typically try to fix this by introducing some form of regularization or just using smaller networks.  All in all [as a maybe-too-stubborn non-deep learner  ;-)  ] I could still wonder what is really the problem...}
Allowing noisy annotations to exist could help significantly increase the number of annotated images and the number of training samples could compensate the impact of annotation errors.
We will further discuss this in the related works.


%%%%%%%%%%%%%%%%% Feature noise robustness
% But dogs and cats have visual features: they are both furry, have similiar shape and appearance, etc.
% These similiarities in features between dogs and cats may correspond to some interm features when a toy dog was misannotated as a dog.
% If we consider further lower features, for exmaple those for detecting edges and corners, they are commonly transferable to many categories, not necessarily ones that look alike.
% dissimilar categories (man-made classes and natural classes)
%If we constraint ourselves to perceive only part of the dog and the toy dog, for example the ears, it may become difficult even for us to distinguish the two because given only the local information there is not much difference between them.
% The generality of the low-level features can introduce the robustness to mis-segmentations regarding the transferability when we want to transfer hieraychical features for semantic segmentation in the presence misannotated objects because the low-level features are not dependent on the exact category assigned to the objects.
% This annotation error robustness for the low-level features could result in noise robustness for the multiple level pre-trained weights for semantic segmentation with noisy labels.
% In Section \ref{sec:objectness}, we tested whether object mis-segmentation had an impact on the transfehen we transfer the learned features to a new dataset with new categories.
% That leads us to the following research question:
% \begin{enumerate}
%   \item How do mis-segmentation and misclassification of instances influence the ``transferability'' of the learned features respectively?
% \end{enumerate}


%%%%%%%%%%%%%%%% Spatial label noise dependency
% \noindent \textit{Clarity for noises considered.}
% $p(\tilde{y_{ij}},x) = p(\tilde{y_{ij}} \vert x, y, e_{ij}) p(e_{ij} \vert x,y)$.
% In the above model, label errorness for pixels is assumed to be independent to each other despite the fact that neighboring pixels are likely to have the same label in practice.
%Conditional random fields (CRF) were introduced to interpret the neighboring pixel dependence.\cite{chen2016deeplab}
% Note that all these label errors apply to the whole segment instead of to individual pixels.
% That is pixels for the same segments will have the same true labels and observed labels.
% Therefore, the above corruption model hides the spatial dependence of $e_{ij}$ from expressions.
% It would be of more value to use a real dataset for such errors than to synthesize.
% The occurrence of error of each pixel is no longer simply dependent on which segments it belongs to but instead dependent on the neighboring pixels.
% How to model the pixel label errorness conditioning on neightboring pixels is not the focus of this paper and is left for future studies.


\subsection{assumptions}
They all assume that positive examples in the positive set P and the positive examples in
the unlabeled set U are generated from the same distribution.


%%%%%%%%%%%%% Bullets
% \section{Bullet points}
%
%
% \paragraph{Data for Semantic Segmatation}
% \begin{itemize}
%   \item Rich unlabeled data, but lack of annotations
%   \item (Conventionally) The annotations need to be:
%   \begin{itemize}
%     \item \textbf{precise}: no false positive
%     \item \textbf{complete}: no false negative
%     \item \textbf{consistent}: no misclassification
%   \end{itemize}
%
%   % \item (Conventional) Workflow:
%   % \begin{enumerate}
%   %   \item Filter out the irrelavant images
%   %   \item Annotate the relavant images
%   %   \item Iterate for multiple times Step 1. and 2.
%   % \end{enumerate}
%
%   \item Annotations that
%   \begin{itemize}
%       \item contain \textbf{imprecise} annotations
%       \item are \textbf{imcomplete}
%       \item contain \textbf{misclassification}
%   \end{itemize}
%
%   % \item Motivation:
%   % \begin{itemize}
%   %   \item Correcting annotation errors take much more effort than annotating more data
%   %   \item Annotation errors could be compensated with appropriate assumptions
%   % \end{itemize}
% \end{itemize}
%
%
% \paragraph{Methods}
% \begin{itemize}
%   \item Incomplete annotations: Positive and Unlabelled learning
%   \item Imprecise annotations: Learning ``objectness''
%   \item Inconsistent classifications: ``Bootstrapping''(Entropy Regularization), Linear Noise model, Dropout Regularization
% \end{itemize}


%%%%%%%% TEXT Encouraging confident predictions
% \paragraph{Confidence of contrary prediction and probability of false negative label}
% \noindent
% \noindent \textit{The idea of alleviating punishment for confident predictions.}
% Losses above assume that the true label of an observed negative label is independent of the inputs $x$.
% However, the true label $y$ is often dependent on both $x$ and $\tilde{y}$ in practice.
% For instance, in segmentation problems, whether a pixel is left unsegmented correctly or not can be determined by the semantical meaning of this pixel and pixels around.
% It is nevertheless difficult to estimate $p(y \vert x, \tilde{y})$ directly due to the difficulty of exploring the joint distribution of $x$ and $\tilde{y}$.
% Alternatively, one can use the probabilistic prediction $p(y \vert x;\theta)$ a classification model to provide extra information about the inputs.
% The predicted probabilities indicate how confident the current model is for the predictions made.
% Given a good enough model, more confident positive predictions for negatively labeled examples can have a higher probability of being wrongly annotated than the less confident ones.
% \footnote{J: This argument needs more detailed explanation.}
% With a random model by which predictions are made randomly, the high and low confidences do not convey any information about the underlying inputs distribution.
% By contrast, confident predictions made by a trained model indicate that the corresponding samples are possibly close to some previous training samples with positive labels in the feature space.
% The natural trend of similar examples having the same labels supports a higher probability of being annotated wrongly for confident predictions which have opposite labels.
% This thought leads to the idea of not punishing confident contrary predictions more than unconfident ones as the traditional logistic/softmax loss will do.

%%%%%%%%% Spatial independent noise assumption
% For example, we stochastically flipped pixel labels for the whole segment, instead of individual pixel label, to synthesize incomplete segmentation errors in Section \ref{sec:robustness},
% Even with annotation errors like under-segmented objects, the probability for a pixel to be unsegmented depends on its neighbor pixels.
% We made a strong assumption of spatial independence for false negative labels for inexhaustive segmentation noises.
% % This assumption allows to keep the problem simple and straightforward but we showed in Section \ref{subsec:pulearning} that this assumption become an issue to recover the whole unsegmented objects rather than individual pixels.
% Future works maybe required to relax this assumption and make use of the spatial dependence of label noises to achieve higher mean intersection over union (mean IU) not only higher accuracy.

%%%%%%%% noises synthesization
% The noisy dataset containing labels for target segments stochastically transited to a random class with probability $p_{jk}=\frac{1}{20}$.
% The resulted trained model was denoted as the AllRandomLabels model in Table \ref{tab:robustness}.
% Similiarly, if a random half of the segmented objects were assigned random labels, the resulting pre-trained model is called the HalfRandomLabels model.
% The noise-free counterpart of these two noisy models was the model trained with true labels, denoting as the TrueLabels model.
% True labels can be considered as labels transiting with probability $p_{jk}=\delta(j-k)$.
% $$p(\tilde{y_{ij}}=l \vert y_{ij}=k) = \delta(l-k)$$.


%%%%%%%% noises synthesization
% The small pre-training dataset can be a reason of similarly performed binary and multi-class pre-trained models.
% In our experiment, most classes had only around one hundred images, and the dining table had only 20 images.
% The limited number of class samples may increase the difficulty to segment individual classes and may explain why binarized labels could achieve comparable fine-tuning performance to true labels.
