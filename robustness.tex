\section{Noise robustness of feature transferability}
\label{sec:robustness}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TEXT Noise model
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent \textit{From feature generality to feature robustness}

\noindent
Convolutional neural networks for images are believed to extract a rich hierarchy of image  features.
Some neurons capture concepts such as people and text while some units capture texture and material properties, such as dot arrays and specular reflections\cite{girshick2014rich}.
The conceptual features are specific for particular categories and the textural features can be shared by various categories.
Furthermore, low-level features were found showing less specialization than the high-level features.
By training a CNN with half of the ImageNet categories and transfer features to the other half, Yosinski et al.\cite{yosinski2014transferable} found feature transferability decreases from the bottom layers to the top layers.
The low level features, especially features of the first two layers, presented outstanding transferability between categories with far distance, for example natural categories and man-made classes as reported in \cite{yosinski2014transferable}.
Another evidence for this is that the first convolutional features for images are often observed as Gabor filters or color blobs even training with different datasets and different objectives\cite{zeiler2014visualizing,lee2009convolutional,krizhevsky2012imagenet,shin2016deep}.
Given the evidence that low-level features can be category independent and shared by categories far from each other, we wondered if they are robust to learn with noisy training labels.
More specifically speaking, the research question concerned in this section is:
\begin{enumerate}
  \item  Is transferability of convolution neural networks negatively influenced by the label noises?
\end{enumerate}

\noindent \textit{Table of contents}
\noindent
We formulated the learning task and annotation errors of interest in Section \ref{subsec:formulation} and described the experiment setups in Section \ref{subsec:robustness} to learn how transferable the features were when they were trained with a noisy dataset.


\subsection{Problem Formulation}
\label{subsec:formulation}

\paragraph{Semantic Segmentation}
\noindent \textit{Segmentation is per-pixel classification}
\noindent
The goal of semantic segmentation tasks is to segment images into semantically meaningful partitions, a.k.a.,\textit{segments}.
These segments are \textit{target} if they depict instances of pre-defined object categories or \textit{non-target} otherwise.
A common interpretation for semantic image segmentation tasks with CNNs is the pixel-wise classification model:
Given an image $x$, a segmenting model $f: R^{h \times w \times c} \rightarrow R^{h \times w}$ predicts a label for each pixel and predict a label map $\hat{y}$ that has the same size as $x$.
$h, w$ are image height and weight respectively, and $c$ is the number of image channels.
The corresponding annotation $y$ consist of labels for each pixel.
Supposing there are $K$ predefined categories, the label of pixel ${ij}$
\[
  y_{ij} =
    \begin{cases}
      k \in [1,K], & \text{for target pixels} \\
      0, & \text{for non-target pixels}
    \end{cases}
\]
where $i \in [1,h], j \in [1,w]$.
% Pixels with $k=0$ are also  \textit{unannotated} since they were not assigned to one of the predefined categories.

\paragraph{Label corruption model}
\noindent \textit{How noises synthesized}
\noindent
A straightforward way to model noisy labels is to corrupt true labels with a corruption model.
The corruption model describes the probability of observed label map conditioning on the the true label map $y$, the image $x$ and label errorness $e$:
$$p(\tilde{y} \vert x, y, e)$$
where the occurence of errors $e$ for pixels depends on the inputs $x$ and true labels $y$.
Such a noise model is called noisy not at random (NNAR) \cite{frenay2014classification} because the noise depends on not only the true label $y$ but also the inputs $x$.
Given a corruption model and true annotations, one can synthesized the corresponding noisy annotations stochastically.


\subsection{Noises in segmentation}
\label{subsec:noises}
\noindent \textit{Clarity for noise considered.}
\noindent
% In the above model, label errorness for pixels is assumed to be independent to each other despite the fact that neighboring pixels are likely to have the same label in practice.
%Conditional random fields (CRF) were introduced to interpret the neighboring pixel dependence.\cite{chen2016deeplab}
In our works, we considered three types of errors for a semantic segmentation problem, \textit{misannotation}, \textit{misclassification} and \textit{inexaustive annotation}, and modified the NNAR model accordingly.
Note that all these label errors apply to the whole segment instead of to individual pixels.
That is pixels for the same segments will have the same true labels and observed labels.
We exluded segmenting errors such as inprecise boundaries, oversegmenting or undersegmenting the objects from study because they are more difficulty to synthesize than the preceding classification errors.
It would be of more value to use a real dataset for such errors than to synthesize.
% The occurrence of error of each pixel is no longer simply dependent on which segments it belongs to but instead dependent on the neighboring pixels.
% How to model the pixel label errorness conditioning on neightboring pixels is not the focus of this paper and is left for future studies.

% $p(\tilde{y_{ij}},x) = p(\tilde{y_{ij}} \vert x, y, e_{ij}) p(e_{ij} \vert x,y)$.


\paragraph{Misannotation}
\noindent
Misannotation denotes the wrongly segmented objects for categories that are semantically meaningful but are not predefined.
For example, a toy dog can be misannotated as a dog, assuming that ``dog'' is predefined and ``toy dog'' is not.
In the presence of misannotation, pixel labels of segment $S$ transit from $0$ to $k$ with probability
$$p(\tilde{y_{ij}}=k \vert x, y_{ij}=0), ij \in S $$
% $\{\tilde{y_{kl}}, kl \in P \setminus \{ij\} \}$
% where P is all the pixels in image $x$, and $ij$, $kl$ are both pixels in the image.
The dependence of observed pixel labels $\tilde{y_{ij}}$ on the original image $x$ interpret the premise that misannotation would only happen to semantically meaningful segments in an image.
It is natural to include this premise because semantically meaningless partitions of an image are less likely to be segmented by an annotator.
However, it is difficult to estimate the above proability in practice because it is conditioning on the semantic meaning of $x$.
Alternatively, we can synthesize misannotation errors by selecting part of the categories as non-target categories so that instances of these categories should have zero labels for correct segmentations.
We can then misannotate these non-target instances stochastically with a simplified probability $p(\tilde{y_{ij}}=k \vert y_{ij}=0)=p_k$ without  interpreting semantic meaning of $x$ in probability.
Note that $p_k$ sums up to 1 for all classes, including class 0, $\sum_0^K p_k = 1$.

\noindent
\paragraph{Misclassification}
Different from misannotation, misclassification error means labels were misclassified between pre-defined categories.
For example, cats may be misclassified as dogs occasionally if both ``cat'' and ``dog'' are target classes.
In the presence of misclassification, pixel labels of segment $S$ are transited from $k$ to $j$ stochastically with probability:
$$p(\tilde{y_{ij}}=j \vert y_{ij}=k) = p_{jk}, ij \in S, k,j \in [1,K]$$
where $\sum_{j=1}^{K}p_{jk}=1$.
We assumed misclassification error is independent of the exact shape and appearance of the objects, i.e.information from $x$.
This model is often called \textit{noisy at random} \cite{frenay2014classification}.
This assumption does not hold in every cases of practice, for example, some instances can be more likely to be misclassifified due to its ambiguity in shapes or apperances.
But the difficulty of modeling the depence of $x$ leads to simply assuming an input indepence.
Given the class transition probabilities, one can easily synthesize noisy annotations including misclassification errors given a well-annotated segmentation dataset.

\noindent
\paragraph{Inexaustive annotation}
Inexaustive annotation denotes that there exists unsegmented instances for pre-defined categories.
Pixels of an unannotated object $S$ have labels flipped from $k$ to $0$ with probability:
$$p(\tilde{y_{ij}}=0\vert y_{ij}=k) = q_k, ij \in S, k \in [1,K]$$
In words, an instance of category $k$ is left unsegmented stochastically with probability $q_k$.
The probability of correctly segmented in annotations is then:
$$p(\tilde{y_{ij}}=k\vert y_{ij}=k) = 1-q_k, ij \in S, k \in [1,K]$$
Again we assumed inexaustive annotations to be noisy at random (NAR).


% The inexhaustive annotations need to be properly handled given the prior knowledge modeling the missing pattern of the annotations.
% Given that we believe any annotated instance provide information, all the foreground pixels that correspond to the annotated instances become reliable and the background pixels may contain both the true background pixels and object pixels unannotated.
% That satisfies a Positive and Unlabeled learning setup where the training dataset contains only the positive examples and unlabeled examples that are the mixed of the positive samples and negative samples.



% Supposing a dog is wrongly annotated as a cat, the error could mislead the model to predict the dog as cat because the learning objective drives the model to fit the given labels.
% But dogs and cats have visual features: they are both furry, have similiar shape and appearance, etc.
% These similiarities in features between dogs and cats may correspond to some interm features when a toy dog was misannotated as a dog.
% If we consider further lower features, for exmaple those for detecting edges and corners, they are commonly transferable to many categories, not necessarily ones that look alike.
% dissimilar categories (man-made classes and natural classes)
%If we constraint ourselves to perceive only part of the dog and the toy dog, for example the ears, it may become difficult even for us to distinguish the two because given only the local information there is not much difference between them.
% The generality of the low-level features can introduce the robustness to misannotations regarding the transferability when we want to transfer hieraychical features for semantic segmentation in the presence misannotated objects because the low-level features are not dependent on the exact category assigned to the objects.
% This annotation error robustness for the low-level features could result in noise robustness for the multiple level pre-trained weights for semantic segmentation with noisy labels.
% In Section \ref{sec:objectness}, we tested whether object misannotation had an impact on the transfehen we transfer the learned features to a new dataset with new categories.
% That leads us to the following research question:
% \begin{enumerate}
%   \item How do misannotation and misclassification of instances influence the ``transferability'' of the learned features respectively?
% \end{enumerate}
